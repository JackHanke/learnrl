# Chapter 3 Notes and Exersizes

## Exercize 3.1
Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.

Playing the mobile game 2048 fits into the MDP framework, as the board state are the states, the actions are the up,down,left, and right moves one can make to change the board, and the reward is the score. 

NOT DONE

## Exercize 3.2
Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?

As the Markov Decision Process requires the Markov property, that being the state must contain all aspects of past interactions that make a difference in the future, any goal-directed learning task that violates this property is an exception. An example of this would be investing in the stock market. The current state of the stock market contains some information that makes a difference in the future of the stock market, but maximizing reward is dependent on every variable that affects the economy, which is impossible to know.   

## Exercize 3.3
Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?

One should prefer a line that is drawn so that the actions, states, and rewards best reflect the environment in which the agent will interact, and so that the agent derives the behavior tha is at the line of the unknown/undescribeable. Given that in the self-driving case, we know the high-level path the car should take thanks to A* and Google Maps and such, we want the undescribeable part of the minute changes to the controls to be the actions the agent will deal in. This leads me to believe that the actions being the accelerator, steering wheel, and brake best reflect the problem of driving. 

## Exercize 3.4
Give a table analogous to that in Example 3.3, but for $p(s' , r | s, a)$. It should have columns for $s, a, s', r$, and $p(s',r|s,a)$, and a row for every 4-tuple for which $p(s',r|s,a) > 0$.

| $s$ | $a$ | $s'$ | $r$ | $p(s',r$ &#124; $s,a)$ |
|---|---|----|---|--------------------|
| high | search | high | $r_{search}$ | $\alpha$ |
| high | search | low | $r_{search}$ | $1-\alpha$ |
| low | search | high | -3 | $1-\beta$ |
| low | search | low | $r_{search}$ | $\beta$ |
| high | wait | high | $r_{wait}$ | 1 |
| low | wait | low | $r_{wait}$ | 1 |

## Exercize 3.5
The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3), that being
$$\sum_{s' \in \cal{S}} \sum_{r \in \cal{R}}p(s', r | s, a) = 1$$

for a given $s \in \cal{S}$ and $a \in \cal{A}(s).$

This just needs to be changed to include the terminal state, ie
$$\sum_{s' \in \cal{S}^{+}} \sum_{r \in \cal{R}}p(s', r | s, a) = 1$$

for a given $s \in \cal{S}^+$ and $a \in \cal{A}(s).$, where if one is in the terminal state any action $a$ results in $s'=s_T$ and $r=0$.

## Exercize 3.6
Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for $-1$ upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?

The reward would be $-\gamma^{T}$, where $T$ is the termination time. 
NOT DONE


## Exercize 3.7
Imagine that you are designing a robot to run a maze. You decide to give it a reward of $+1$ for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?

As there is no discounting, you have not effectively communicated to the agent that the maze is best solved quickly. Using $3.7$, the maze solved at $t=10$ receives the same amount of return as $t=10^{10}$, that being $1$. Discounting encourages solving the maze quickly to better maximize it's reward. 

## Exercize 3.8
Suppose $\gamma = 0.5$ and the following sequence of rewards is received $R_1 = -1, R_2 =2,R_3 =6,R_4 =3$,and $R_5 =2$, with $T =5$. What are $G_0,G_1,...,G_5$ ? Hint: Work backwards.

$G_0=2,G_1=6,G_2=8,G_3=4,G_4=2,G_5=0.$

## Exercize 3.9
Suppose $\gamma = 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite sequence of $7$ s. What are $G_1$ and $G_0$?

$G_0 = 65$
$G_1 = 70$

## Exercize 3.10
$$\sum_{k=0}^{\infty}\gamma^k = \lim_{N \to \infty}\sum_{k=0}^{N}\gamma^k = \lim_{N \to \infty} \frac{1-\gamma^{N+1}}{1-\gamma} = \frac{1}{1-\gamma}$$

## Section 3.5 Policies and Value Functions

The *value function* of a state $s$ under policy $\pi$, denoted $v_{\pi}(s)$, is the expected return when starting in $s$ and following $\pi$ thereafter. For MDPs, we have the second equality

$$v_{\pi}(s) := \mathbb{E}_{\pi}(G_t|S_t=s)=\mathbb{E}_{\pi}\left(\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s \right)$$

Similarly, we define the value of taking action $a$ in state $s$under a policy $\pi$, denoted $q_{\pi}(s,a)$, as the expected return starting from $s$, taking action $a$, and thereagter following $\pi$. 

$$q_{\pi}(s,a) := \mathbb{E}_{\pi}(G_t|S_t=s, A_t=a)=\mathbb{E}_{\pi}\left(\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s, A_t=a \right)$$

We call $q_{\pi}$ the action value function for policy $\pi$.

A fundamental property of value functions is that they satisfy recurrence relationships similar to the return recurence relationship. 

$
\begin{aligned}
v_{\pi}(s) &:= E_{\pi}(G_t|S_t=s) \\
&= E_{\pi}(R_{t+1} + \gamma G_{t+1}|S_t=s) \\
&= \sum_{a \in \cal{A}(s)}\pi(a|s)\sum_{s'}\sum_{r}p(s',r|s,a)\left(r + \gamma \mathbb{E}(G_{t+1}|S_t=s')\right) \\
&= \sum_{a \in \cal{A}(s)}\pi(a|s)\sum_{s'}\sum_{r}p(s',r|s,a)\left(r + \gamma v_{\pi}(s') \right)
\end{aligned}
$

These are called the *Bellman Equations* for $v_{\pi}$, specifically of which $v_{\pi}$ is the unique solution.

## Exercize 3.11
$$\mathbb{E}_{\pi}[R_{t+1}|S_t=s] = \sum_{s',r}r\sum_{a \in \cal{A}(s)}p(s',r|s,a)\pi(a|s)$$

## Exercize 3.12
Give an equation for $v_{\pi}$ in terms of $q_{\pi}$ and $\pi$

$$v_\pi(s) = \sum_{a \in \cal{A}(s)} q_{\pi}(s,a) \pi(a|s)$$

## Exercize 3.13
Give an equation for $q_{\pi}$ in terms of $v_{\pi}$ and $p(s',r|s,a)$.

$$q_{\pi}(s,a) = \sum_{s',r}r v_{\pi}(s')p(s',r|s,a)$$

## Exercize 3.14

$
\begin{aligned}
v_{\pi}((2,2)) &= \sum_{a \in (left, right, up, down)}\pi(a|s)\sum_{s' \in ((2,1),(2,3),(1,2),(3,2))}\sum_{r}p(s',r|s,a)\left(r + \gamma v_{\pi}(s')\right) \\
&= \sum_{a \in \cal{A}(s)}\frac{1}{4}\sum_{s'}\left(0.9 v_{\pi}(s') \right) \\
&= \frac{9}{40}(0.7 + 0.4 + 2.3 + -0.4) = \frac{27}{40} = 0.675
\end{aligned}
$

## Exercize 3.15

$$\sum_{k=0}^{\infty}\gamma^k (R_{t+k+1} + c) = \frac{c}{1-\gamma}+\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}$$

## Exercize 3.16
Adding a constant $c$ in an episodic task results in a non-constant $v_c$ of $\frac{c(1-\gamma^{T+1})}{1-\gamma}$, where $T$ is the termination time. This would add extra reward to tasks that terminate quickly. For a maze-running task this may be beneficial (though this term should be ensured to not be so large that it overpowers the signal for getting out of the maze), but an episodic task where it is beneficial to have a large $T$ would be hurt by the addition of a constant. 

## Exercize 3.17
$
\begin{aligned}
q_{\pi}(s,a) &:= E_{\pi}(G_t|S_t=s, A_t=a) \\
&= E_{\pi}(R_{t+1} + \gamma G_{t+1}|S_t=s,A_t=a) \\
&= \sum_{s' \in \cal{S}}\sum_{r}p(s',r|s,a)\left(r + \gamma \mathbb{E}(G_{t+1}|S_t=s')\right) \\
&= \sum_{s' \in \cal{S}}\sum_{r}p(s',r|s,a)\left(r + \gamma \sum_{a' \in \cal{A}(s')}\pi(a'|s')q_{\pi}(s',a') \right)
\end{aligned}
$

## Exercize 3.18
$$v_{\pi}(s) := \mathbb{E}_{\pi}(G_t|S_t=s)=\sum_{a \in \cal{A}(s)}\pi(a|s)v_{\pi}(s,a)$$

## Exercize 3.19
$
\begin{aligned}
q_{\pi}(s,a) &:= \mathbb{E}(R_{t+1}|S_t = s, A_t = a) + \gamma \mathbb{E} (v_{\pi}(S_{t+1})|S_t = s, A_t = a) \\
&= \sum_{(s',r)}rp(s',r|s,a)  + \gamma \sum_{s'}v_{\pi}(s')
\end{aligned}
$

## Exercize 3.20

## Exercize 3.21

## Exercize 3.22
$\pi_{left}$ is optimal if $\gamma=0$, as the total return is equivalent to the next reward, which is larger on the left branch than it is on the right. 

The return for $\pi_{left}$ when $\gamma=0.9$ is 
$$0.9 + 0.9^3 + 0.9^5 + \dots = \frac{0.9}{1-0.81}$$
and the return for $\pi_{right}$ when $\gamma=0.9$ is
$$2*0.9^2 + 2*0.9^4 + 2*0.9^6 + \dots = \frac{2*0.81}{1-0.81}$$
Therefore $\pi_{right}$ is optimal for $\gamma=0.9$. However, for $\gamma=0.5$, either policy has the same return. 

## Exercize 3.23


## Exercize 3.24
$v_*((0,1)) = r + \gamma v_*((4,1))$

$10 + 0.9*16 = 24.4$

## Exercize 3.25


## Exercize 3.26


## Exercize 3.27


## Exercize 3.28


## Exercize 3.29

## Questions
Value vs return?