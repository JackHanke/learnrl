# Chapter 3 Notes and Excersizes

## Exercize 3.1
Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.

Playing the mobile game 2048 fits into the MDP framework, as the board state are the states, the actions are the up,down,left, and right moves one can make to change the board, and the reward is the score. 

NOT DONE

## Exercize 3.2
Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?

As the Markov Decision Process requires the Markov property, that being the state must contain all aspects of past interactions that make a difference in the future, any goal-directed learning task that violates this property is an exception. An example of this would be investing in the stock market. The current state of the stock market contains some information that makes a difference in the future of the stock market, but maximizing reward is dependent on every variable that affects the economy, which is impossible to know.   

## Exercize 3.3
Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?

One should prefer a line that is drawn so that the actions, states, and rewards best reflect the environment in which the agent will interact, and so that the agent derives the behavior tha is at the line of the unknown/undescribeable. Given that in the self-driving case, we know the high-level path the car should take thanks to A* and Google Maps and such, we want the undescribeable part of the minute changes to the controls to be the actions the agent will deal in. This leads me to believe that the actions being the accelerator, steering wheel, and brake best reflect the problem of driving. 

## Exercize 3.4
Give a table analogous to that in Example 3.3, but for $p(s' , r | s, a)$. It should have columns for $s, a, s', r$, and $p(s',r|s,a)$, and a row for every 4-tuple for which $p(s',r|s,a) > 0$.

| $s$ | $a$ | $s'$ | $r$ | $p(s',r$ &#124; $s,a)$ |
|---|---|----|---|--------------------|
| high | search | high | $r_{search}$ | $\alpha$ |
| high | search | low | $r_{search}$ | $1-\alpha$ |
| low | search | high | -3 | $1-\beta$ |
| low | search | low | $r_{search}$ | $\beta$ |
| high | wait | high | $r_{wait}$ | 1 |
| low | wait | low | $r_{wait}$ | 1 |

## Exercize 3.5
The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3), that being
$$\sum_{s' \in \cal{S}} \sum_{r \in \cal{R}}p(s', r | s, a) = 1$$

for a given $s \in \cal{S}$ and $a \in \cal{A}(s).$

This just needs to be changed to include the terminal state, ie
$$\sum_{s' \in \cal{S}^{+}} \sum_{r \in \cal{R}}p(s', r | s, a) = 1$$

for a given $s \in \cal{S}^+$ and $a \in \cal{A}(s).$, where if one is in the terminal state any action $a$ results in $s'=s_T$ and $r=0$.

## Exercize 3.6
Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for $-1$ upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?

The reward would be $-\gamma^{T}$, where $T$ is the termination time. 
NOT DONE


## Exercize 3.7
Imagine that you are designing a robot to run a maze. You decide to give it a reward of $+1$ for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?

As there is no discounting, you have not effectively communicated to the agent that the maze is best solved quickly. Using $3.7$, the maze solved at $t=10$ receives the same amount of return as $t=10^{10}$, that being $1$. Discounting encourages solving the maze quickly to better maximize it's reward. 

## Exercize 3.8
Suppose $\gamma = 0.5$ and the following sequence of rewards is received $R_1 = -1, R_2 =2,R_3 =6,R_4 =3$,and $R_5 =2$, with $T =5$. What are $G_0,G_1,...,G_5$ ? Hint: Work backwards.

$G_0=2,G_1=6,G_2=8,G_3=4,G_4=2,G_5=0.$

## Exercize 3.9
Suppose $\gamma = 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite sequence of $7$ s. What are $G_1$ and $G_0$?

$G_0 = 65$
$G_1 = 70$

## Exercize 3.10

$$\sum_{k=0}^{\infty}\gamma^k = \lim_{N \to \infty}\sum_{k=0}^{N}\gamma^k = \lim_{N \to \infty} \frac{1-\gamma^{N+1}}{1-\gamma} = \frac{1}{1-\gamma}$$

## Questions
Value vs return?