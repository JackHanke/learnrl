# Chapter 2 Notes and Exersizes

## 2.1 The $k$-armed Bandit Problem

You are faced with a repeated choice among $k$ different actions. After each choice you receive a numerical reward chosen from a stationary probability distribution dependent on the action you selected. The objective is to maximize the expected total reward over some time period.

This is the original $k$ armed bandit problem, analagous to a slot machine (or "one-armed bandit"). Each action is like a play of one of $k$ slot machines. Through repeated action selections you maximize your winnings by concentrationg your actions on the best lever. 

Each of the $k$ actions has an expected reward given the acion is selected. This is the *value* of the action. Denote the action selected at time $t$ as $A_t$, and the corresponding reward as $R_t$. The value of an arbitrary action $a$, denoted $q_*(a)$, is

$$q_*(a) := \mathbb{E}(R_t | A_t=a).$$

We assume this value function is unknown to the agent. Inatead, you estimate the value function over the time steps you experience. We denote the estimated value of the action $a$ at time $t$ to be $Q_t(a)$. 

If one maintains estimates of the action values, then at any time step there is at least one action that has the largest estimated value. We call these the *greedy* actions. When you select one of these actions, we say that you are *exploiting* your current knowledge of he values of the actions. If instead you chose one of the nongreedy actions, then we say you are *exploring*. Exploration may produce greater total reward in the long run, as opposed to just performing the greedy action. 

## 2.2 Action-value Methods

We look at methods for estimating the values of actions and using the estimates to make action selection decisions. We call this *action-value methods*. 

A natural way to estimate the action value function is by taking the sample average of rewards for each action. There are other ways to estimate the values, but for now turn to how he estimates might be used to select actions. 

As a purely greedy method spends no time sampling seemingly inferior but possibly superior disributions, we instead decide to behave greedily with probability $1-\epsilon$ and randomly with probability $\epsilon$. This is an $\epsilon$-greedy mehod. 

## Exercise 2.1

In the case of $2$ actions and $\epsilon=0.5$, the probability that the greedy action is selected is $0.5$ if the $\epsilon$ action excludes the greedy options for it's possible actions, and $0.75$ if not. In general an $\epsilon$-greedy algorithm with known action-value functions choses the correct action with probabiliy $(1-\epsilon) + \frac{\epsilon}{k}$.

In general the smaller epsilon takes longer to learn but has a larger reward ceiling. If we suppose that bandit task were nonstationary, meaning the true values of the acions changes over time, exploration is needed even in the deterministic case to make sure one of the greedy acitons has not chaed to become better. Nonstationarity is is the case most commonly encounted in RL. 

## Exercise 2.2

Consider $k=4$ bandit problem, with $\epsilon$ greedy action selection, sample-average action-value estimates, and all initial estimates $0$. $t=3,4$ are definitely $\epsilon$ steps, and $t=1$ is possibly an $\epsilon$ step. 

## Exercise 2.3

In general an $\epsilon$-greedy algorithm with known action-value functions choses the correct action with probabiliy $(1-\epsilon)$. A given $\epsilon_1$ algorithm picks the correct action $\frac{1-\epsilon_1}{1-\epsilon_2}$ times more than an $\epsilon_2$ algorithm. The average reward in the long run is therefore $(1-\epsilon) * v_*(a_*) + $ the average of the other rewards times $\epsilon$.

## Exercise 2.4
For a specific action we have
$$Q_{n+1} =  \prod_{i=1}^n (1-\alpha_i)Q_1 + \alpha_n R_n + \sum_{i=1}^{n-1} \prod_{j=i}^n (1-\alpha_j) \alpha_i R_i.$$

## Exercise 2.5
Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the $q_*(a)$ start out equal and then take independent random walks (say by adding a normally distributed increment with mean 0 and standard deviation 0.01 to all the $q_*(a)$ on each step). Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, $\alpha = 0.1$. Use $\epsilon = 0.1$ and longer runs, say of $10,000$ steps.

See `2.py`. 

## Exercise 2.6
The spike is due to the fact that after the $k$-th time step of a optimistic initialization, the probability that the highest average is the optimal action is the same as the probability that given a single sample from each distribution, the sample that is the largest is from the largest centered distribution. The assumption I make is that this probability is high (order statistics calculation required to see how high) enough to make a significant spike. This explains the increase in percentage of optimal move selection at the $k+1$-th step. As the method is pure greedy, another sample of this distribution further decreases the average, all-but guaranteeing that the $k+2$-th step does not choose the optimal action again. It is likely that the severity of this spike is inversely related to the size of $k$, as the likelihood of that initial probability would decrease with more distributions to overpower the effect. 

## Exercise 2.7
TODO

## Exercise 2.8
In Figure 2.4 the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: If $c = 1$, then the spike is less prominent. 

This is similar to Exercise 2.6, where the first $k=10$ steps are spent exploring the first $k=10$ actions, and so the probability the optimal action is chosen on $t=k+1$ is $\rho = \mathbb{P}(max(x_1,\dots,x_{10}) = x_{a_*}|X_1=x_1, \dots, X_{10}=x_{10}) = \int_{\mathbb{R}} \mathbb{P}(X_{a_*}=x)\prod_{k \neq a_*}\mathbb{P}(X_k < x)dx$. Again it is assumed that $\rho$ is large, so the probability the optimal action taken is high. However at $t=k+2=12$, the upper confidence bound leaves the optimal action with a small increase in estimated value, but all other actions with a more significant increase in their estimated value. The term $c$ loosely means how much the uncertainty should be taken in to account, and so the size of the drop-off should decrease as the size of $c$ decreases. 

## Exercise 2.9

$$\frac{e^{x_1}}{e^{x_1} + e^{x_2}} = \frac{1}{1 + e^{-(x_1 - x_2)}} = \sigma(x_1-x_2)$$

## Exercise 2.10
Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions 1 and 2 are respectively 10 and 20 with probability 0.5 (case A), and 90 and 80 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expected reward you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still donâ€™t know the true action values). This is an associative search task. What is the best expected reward you can achieve in this task, and how should you behave to achieve it?

$\mathbb{E}(\text{Action 1}) = (\frac{1}{2}10 + \frac{1}{2}90) = 50$

$\mathbb{E}(\text{Action 2}) = (\frac{1}{2}20 + \frac{1}{2}80) = 50$

Without knowing what case you are facing, both Actions result in a maximum reward of $50$. 

However, 

$\mathbb{E}(\text{Action 1 | Case 1}) = 10$

$\mathbb{E}(\text{Action 2 | Case 1}) = 20$

$\mathbb{E}(\text{Action 1 | Case 2}) = 90$

$\mathbb{E}(\text{Action 2 | Case 2}) = 80$

The best strategy knowing what case you are facing is to choose Action 2 if you are in Case 1, and Action 1 if you are in Case 2. In this case the best expected reward is $\frac{90}{2} + \frac{20}{2} = 55.E$

## Exercise 2.11
See `2.py`. TODO